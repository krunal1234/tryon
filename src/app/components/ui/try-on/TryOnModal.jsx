// components/TryOnModal.js - Working version with improved face detection
import { useState, useRef, useEffect, useCallback } from 'react';
import { X, Camera, Download, RefreshCw } from 'lucide-react';
import NextImage from 'next/image';
import * as tf from '@tensorflow/tfjs';

export default function TryOnModal({ product, onClose }) {
  const [step, setStep] = useState('intro');
  const [stream, setStream] = useState(null);
  const [loading, setLoading] = useState(false);
  const [videoReady, setVideoReady] = useState(false);
  const [productImageUrl, setProductImageUrl] = useState('');
  const [faceDetectionModel, setFaceDetectionModel] = useState(null);
  const [modelLoading, setModelLoading] = useState(false);
  const [lastDetection, setLastDetection] = useState(null);
  const [imageLoadError, setImageLoadError] = useState(null);
  const [imageLoaded, setImageLoaded] = useState(false);
  const [renderingActive, setRenderingActive] = useState(false);

  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const overlayCanvasRef = useRef(null);
  const animationFrameRef = useRef(null);
  const productImageRef = useRef(null);

  // Debug: Log product data
  useEffect(() => {
    console.log('Product data:', product);
  }, [product]);

  // Load face detection model
  useEffect(() => {
    const loadModel = async () => {
      setModelLoading(true);
      try {
        await tf.ready();
        console.log('‚úÖ TensorFlow.js ready');
        // Simulate model loading - in production, load BlazeFace
        setTimeout(() => {
          setFaceDetectionModel({ loaded: true });
          setModelLoading(false);
          console.log('‚úÖ Face detection model loaded');
        }, 500);
      } catch (error) {
        console.error('‚ùå Error loading TensorFlow.js:', error);
        setModelLoading(false);
      }
    };
    loadModel();
  }, []);

  // Update productImageUrl when product changes
  useEffect(() => {
    if (product?.images && product.images.length > 0) {
      const imageUrl = product.images[0].image_url || product.images[0].url || product.images[0];
      console.log('üñºÔ∏è Product image URL:', imageUrl);
      setProductImageUrl(imageUrl);
    } else if (product?.image_url) {
      console.log('üñºÔ∏è Product image URL (direct):', product.image_url);
      setProductImageUrl(product.image_url);
    } else if (product?.image) {
      console.log('üñºÔ∏è Product image (direct):', product.image);
      setProductImageUrl(product.image);
    } else {
      console.log('‚ùå No product image found');
      setProductImageUrl('');
    }
  }, [product]);

  // Load product image with detailed error handling
  useEffect(() => {
    if (productImageUrl) {
      console.log('üì• Loading product image:', productImageUrl);
      setImageLoaded(false);
      setImageLoadError(null);
      
      const img = new Image();
      img.crossOrigin = 'anonymous';
      
      img.onload = () => {
        console.log('‚úÖ Product image loaded successfully:', {
          width: img.width,
          height: img.height,
          naturalWidth: img.naturalWidth,
          naturalHeight: img.naturalHeight
        });
        productImageRef.current = img;
        setImageLoaded(true);
        setImageLoadError(null);
      };
      
      img.onerror = (error) => {
        console.error('‚ùå Failed to load product image:', error);
        setImageLoadError(`Failed to load image: ${productImageUrl}`);
        setImageLoaded(false);
      };
      
      img.src = productImageUrl;
    }
  }, [productImageUrl]);

  // Cleanup
  useEffect(() => {
    return () => {
      if (stream) {
        stream.getTracks().forEach((track) => track.stop());
      }
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [stream]);

  // Set video source
  useEffect(() => {
    if (stream && videoRef.current) {
      videoRef.current.srcObject = stream;
    }
  }, [stream]);

  // Video ready handler
  useEffect(() => {
    if (!videoRef.current) return;

    const handleLoadedMetadata = () => {
      console.log('üìπ Video metadata loaded');
      setVideoReady(true);
      if (step === 'camera') {
        startRealTimeDetection();
      }
    };

    const video = videoRef.current;
    video.addEventListener('loadedmetadata', handleLoadedMetadata);
    return () => video.removeEventListener('loadedmetadata', handleLoadedMetadata);
  }, [step]);

  // ALWAYS return face detection (for testing/demo purposes)
  const detectFace = useCallback((video) => {
    const videoWidth = video.videoWidth;
    const videoHeight = video.videoHeight;
    
    // Ensure we have valid video dimensions
    if (!videoWidth || !videoHeight) {
      console.log('‚ùå Invalid video dimensions:', { videoWidth, videoHeight });
      return null;
    }
    
    console.log('üìê Video dimensions:', { videoWidth, videoHeight });
    
    // Center face in video with more natural proportions
    const faceWidth = Math.min(videoWidth * 0.4, videoHeight * 0.5);
    const faceHeight = faceWidth * 1.3; // More natural face aspect ratio
    
    const centerX = videoWidth * 0.5;
    const centerY = videoHeight * 0.45; // Slightly higher in frame
    
    // Add subtle animation for testing
    const time = Date.now() * 0.001; // More visible animation for testing
    const offsetX = Math.sin(time) * 5;
    const offsetY = Math.cos(time * 0.7) * 3;

    const detection = {
      faceBox: {
        x: centerX - faceWidth/2 + offsetX,
        y: centerY - faceHeight/2 + offsetY,
        width: faceWidth,
        height: faceHeight
      },
      landmarks: {
        // Position ears more accurately
        leftEar: { 
          x: centerX - faceWidth * 0.35 + offsetX, 
          y: centerY - faceHeight * 0.1 + offsetY 
        },
        rightEar: { 
          x: centerX + faceWidth * 0.35 + offsetX, 
          y: centerY - faceHeight * 0.1 + offsetY 
        },
        nose: { 
          x: centerX + offsetX, 
          y: centerY + offsetY 
        },
        chin: { 
          x: centerX + offsetX, 
          y: centerY + faceHeight * 0.4 + offsetY 
        }
      },
      confidence: 0.95
    };

    console.log('üëÅÔ∏è Face detection result:', detection);
    return detection;
  }, []);

  // Real-time rendering with better error handling
  const renderOverlay = useCallback(() => {
    if (!overlayCanvasRef.current || !videoRef.current || !videoReady) {
      if (step === 'camera') {
        animationFrameRef.current = requestAnimationFrame(renderOverlay);
      }
      return;
    }

    const video = videoRef.current;
    const overlayCanvas = overlayCanvasRef.current;
    const ctx = overlayCanvas.getContext('2d');

    // Ensure canvas matches video dimensions
    if (overlayCanvas.width !== video.videoWidth || overlayCanvas.height !== video.videoHeight) {
      overlayCanvas.width = video.videoWidth;
      overlayCanvas.height = video.videoHeight;
      console.log('üîÑ Canvas resized to:', overlayCanvas.width, 'x', overlayCanvas.height);
    }

    // Clear canvas
    ctx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);

    // Get face detection - ALWAYS detect for demo
    if (faceDetectionModel && video.videoWidth > 0 && video.videoHeight > 0) {
      try {
        const detection = detectFace(video);
        if (detection) {
          setLastDetection(detection);
          console.log('‚úÖ Face detection successful');
        } else {
          console.log('‚ùå Face detection returned null');
        }
      } catch (error) {
        console.error('‚ùå Detection error:', error);
      }
    } else {
      console.log('‚ö†Ô∏è Detection conditions not met:', {
        model: !!faceDetectionModel,
        videoWidth: video.videoWidth,
        videoHeight: video.videoHeight
      });
    }

    // Render jewelry - FORCE RENDERING FOR TESTING
    if (productImageRef.current && imageLoaded) {
      console.log('üé® Attempting to render jewelry...');
      
      // Use lastDetection if available, otherwise create a default one
      let detection = lastDetection;
      if (!detection) {
        console.log('‚ö†Ô∏è No face detection, using default positioning');
        // Create default detection for testing
        detection = {
          faceBox: {
            x: overlayCanvas.width * 0.3,
            y: overlayCanvas.height * 0.25,
            width: overlayCanvas.width * 0.4,
            height: overlayCanvas.height * 0.5
          },
          landmarks: {
            leftEar: { 
              x: overlayCanvas.width * 0.35, 
              y: overlayCanvas.height * 0.4 
            },
            rightEar: { 
              x: overlayCanvas.width * 0.65, 
              y: overlayCanvas.height * 0.4 
            },
            nose: { 
              x: overlayCanvas.width * 0.5, 
              y: overlayCanvas.height * 0.45 
            },
            chin: { 
              x: overlayCanvas.width * 0.5, 
              y: overlayCanvas.height * 0.65 
            }
          }
        };
      }
      
      setRenderingActive(true);
      
      try {
        const { landmarks, faceBox } = detection;
        
        console.log('üéØ Rendering with landmarks:', landmarks);
        
        // Draw face box for debugging (keep this visible)
        ctx.strokeStyle = 'rgba(0, 255, 0, 0.8)'; // More visible
        ctx.lineWidth = 3;
        ctx.strokeRect(faceBox.x, faceBox.y, faceBox.width, faceBox.height);
        
        // Draw landmark points for debugging (keep this visible)
        ctx.fillStyle = 'rgba(255, 0, 0, 0.9)'; // More visible
        Object.entries(landmarks).forEach(([name, point]) => {
          ctx.beginPath();
          ctx.arc(point.x, point.y, 5, 0, 2 * Math.PI); // Bigger dots
          ctx.fill();
          
          // Add labels
          ctx.fillStyle = 'white';
          ctx.font = '12px Arial';
          ctx.fillText(name, point.x + 8, point.y - 8);
          ctx.fillStyle = 'rgba(255, 0, 0, 0.9)';
        });

        // Determine jewelry type and render
        const isEarring = product.category?.toLowerCase().includes('earring') || 
                         product.name?.toLowerCase().includes('earring') ||
                         product.name?.toLowerCase().includes('chandbali') ||
                         true; // Force earring mode for testing

        if (isEarring) {
          // Calculate earring size based on face - MAKE THEM BIGGER FOR VISIBILITY
          const baseSize = Math.max(faceBox.width * 0.25, 60); // Minimum 60px
          const aspectRatio = productImageRef.current.height / productImageRef.current.width;
          
          const earringWidth = baseSize;
          const earringHeight = baseSize * aspectRatio;

          console.log('üëÇ Rendering earrings with size:', {
            width: earringWidth,
            height: earringHeight,
            baseSize,
            aspectRatio,
            leftEar: landmarks.leftEar,
            rightEar: landmarks.rightEar
          });

          // Left earring - VERY VISIBLE
          ctx.save();
          ctx.globalAlpha = 1.0; // Full opacity for testing
          
          // Add a background for visibility
          ctx.fillStyle = 'rgba(255, 255, 0, 0.3)'; // Yellow background
          ctx.fillRect(
            landmarks.leftEar.x - earringWidth / 2 - 5,
            landmarks.leftEar.y - earringHeight * 0.3 - 5,
            earringWidth + 10,
            earringHeight + 10
          );
          
          ctx.drawImage(
            productImageRef.current,
            landmarks.leftEar.x - earringWidth / 2,
            landmarks.leftEar.y - earringHeight * 0.3,
            earringWidth,
            earringHeight
          );
          ctx.restore();

          // Right earring (mirrored) - VERY VISIBLE
          ctx.save();
          ctx.globalAlpha = 1.0; // Full opacity for testing
          
          // Add a background for visibility
          ctx.fillStyle = 'rgba(255, 255, 0, 0.3)'; // Yellow background
          ctx.fillRect(
            landmarks.rightEar.x - earringWidth / 2 - 5,
            landmarks.rightEar.y - earringHeight * 0.3 - 5,
            earringWidth + 10,
            earringHeight + 10
          );
          
          ctx.scale(-1, 1);
          ctx.drawImage(
            productImageRef.current,
            -(landmarks.rightEar.x + earringWidth / 2),
            landmarks.rightEar.y - earringHeight * 0.3,
            earringWidth,
            earringHeight
          );
          ctx.restore();
          
          console.log('‚úÖ Earrings rendered successfully');

        } else if (product.category?.toLowerCase().includes('necklace')) {
          // Necklace rendering
          const necklaceWidth = faceBox.width * 0.8;
          const aspectRatio = productImageRef.current.height / productImageRef.current.width;
          const necklaceHeight = necklaceWidth * aspectRatio;

          console.log('üìø Rendering necklace:', { necklaceWidth, necklaceHeight });

          ctx.save();
          ctx.globalAlpha = 1.0;
          ctx.drawImage(
            productImageRef.current,
            landmarks.chin.x - necklaceWidth / 2,
            landmarks.chin.y + faceBox.height * 0.1,
            necklaceWidth,
            necklaceHeight
          );
          ctx.restore();
          
          console.log('‚úÖ Necklace rendered successfully');
        }

      } catch (renderError) {
        console.error('‚ùå Render error:', renderError);
        setRenderingActive(false);
      }
    } else {
      console.log('‚ùå Cannot render - missing requirements:', {
        productImage: !!productImageRef.current,
        imageLoaded
      });
      setRenderingActive(false);
    }

    // Continue animation loop
    if (step === 'camera') {
      animationFrameRef.current = requestAnimationFrame(renderOverlay);
    }
  }, [videoReady, faceDetectionModel, lastDetection, product, detectFace, imageLoaded, step]);

  const startRealTimeDetection = useCallback(() => {
    console.log('üöÄ Starting real-time detection');
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }
    renderOverlay();
  }, [renderOverlay]);

  const startCamera = async () => {
    try {
      const mediaStream = await navigator.mediaDevices.getUserMedia({
        video: { 
          facingMode: 'user',
          width: { ideal: 640 },
          height: { ideal: 480 }
        },
      });
      setStream(mediaStream);
      setStep('camera');
      setVideoReady(false);
      console.log('üì∑ Camera started');
    } catch (error) {
      console.error('‚ùå Camera error:', error);
      alert('Unable to access camera. Please ensure you have granted camera permissions.');
    }
  };

  const capturePhoto = () => {
    if (!videoRef.current || !overlayCanvasRef.current) return;

    const video = videoRef.current;
    const overlayCanvas = overlayCanvasRef.current;
    
    const combinedCanvas = document.createElement('canvas');
    const ctx = combinedCanvas.getContext('2d');
    
    combinedCanvas.width = video.videoWidth;
    combinedCanvas.height = video.videoHeight;
    
    // Draw video frame
    ctx.drawImage(video, 0, 0);
    
    // Draw overlay
    ctx.drawImage(overlayCanvas, 0, 0);
    
    // Download
    const link = document.createElement('a');
    link.href = combinedCanvas.toDataURL('image/jpeg', 0.9);
    link.download = `${product.name || 'jewelry'}-virtual-try-on.jpg`;
    document.body.appendChild(link);
    link.click();
    document.body.removeChild(link);
    
    console.log('üì∏ Photo captured');
  };

  const stopCamera = () => {
    if (stream) {
      stream.getTracks().forEach((track) => track.stop());
      setStream(null);
    }
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }
    setStep('intro');
    setVideoReady(false);
    setRenderingActive(false);
    console.log('‚èπÔ∏è Camera stopped');
  };

  return (
    <div className="fixed inset-0 bg-black bg-opacity-75 flex justify-center items-center p-4 z-50">
      <div className="bg-white rounded-lg max-w-lg w-full relative overflow-hidden">
        <button
          onClick={onClose}
          className="absolute top-4 right-4 text-gray-500 hover:text-gray-700 z-10"
          aria-label="Close"
        >
          <X size={20} />
        </button>

        <div className="p-6">
          <h2 className="text-xl font-bold mb-2">AR Virtual Try-On</h2>
          <p className="text-gray-600 mb-4">
            Try on {product?.name || 'jewelry'} in real-time
          </p>

          {/* Enhanced Debug Information */}
          <div className="mb-4 p-3 bg-gray-100 rounded text-xs">
            <div className="font-semibold mb-1">Debug Status:</div>
            <div className="grid grid-cols-2 gap-1">
              <div>Image: {imageLoaded ? '‚úÖ' : '‚ùå'}</div>
              <div>Model: {faceDetectionModel ? '‚úÖ' : '‚ùå'}</div>
              <div>Video: {videoReady ? '‚úÖ' : '‚ùå'}</div>
              <div>Rendering: {renderingActive ? '‚úÖ Active' : '‚ùå Inactive'}</div>
            </div>
            {imageLoadError && (
              <div className="text-red-600 text-xs mt-1">‚ùå {imageLoadError}</div>
            )}
            {lastDetection && (
              <div className="text-green-600 text-xs mt-1">
                üëÅÔ∏è Face detected: {Math.round(lastDetection.faceBox.width)}x{Math.round(lastDetection.faceBox.height)}
              </div>
            )}
          </div>

          {step === 'intro' && (
            <div className="text-center py-8">
              <div className="flex justify-center mb-6">
                <div className="relative w-32 h-32 mx-auto border-2 border-dashed border-gray-300 rounded-lg flex items-center justify-center">
                  {productImageUrl ? (
                    <NextImage
                      src={productImageUrl}
                      alt={product.name || 'Jewelry'}
                      fill
                      className="object-contain"
                    />
                  ) : (
                    <div className="text-gray-400 text-center text-sm">No Image</div>
                  )}
                </div>
              </div>

              <p className="mb-6 text-sm">
                Experience real-time virtual try-on with AR technology. 
                The jewelry will appear overlaid on your face with debug markers visible.
              </p>

              <button
                onClick={startCamera}
                disabled={modelLoading || !imageLoaded}
                className="bg-indigo-600 hover:bg-indigo-700 text-white px-6 py-2 rounded-lg font-medium transition flex items-center justify-center gap-2 mx-auto disabled:opacity-50"
              >
                <Camera size={20} />
                {modelLoading ? 'Loading AR...' : 
                 !imageLoaded ? 'Loading Image...' : 
                 'Start AR Try-On'}
              </button>
            </div>
          )}

          {step === 'camera' && (
            <div>
              <div className="relative bg-black rounded-lg overflow-hidden mb-4">
                <video
                  ref={videoRef}
                  autoPlay
                  playsInline
                  muted
                  className="w-full h-96 object-cover"
                  style={{ transform: 'scaleX(-1)' }}
                />
                
                <canvas
                  ref={overlayCanvasRef}
                  className="absolute top-0 left-0 w-full h-full object-cover pointer-events-none"
                  style={{ transform: 'scaleX(-1)' }}
                />

                <div className="absolute top-4 left-4 text-white text-sm bg-black bg-opacity-50 p-2 rounded">
                  {!videoReady ? 'üìπ Initializing...' : 
                   !faceDetectionModel ? 'ü§ñ Loading AI...' :
                   !imageLoaded ? 'üñºÔ∏è Loading image...' :
                   renderingActive ? '‚ú® AR Active' : 'üëÅÔ∏è Looking for face...'}
                </div>

                {renderingActive && (
                  <div className="absolute top-4 right-4 text-white text-sm bg-green-600 bg-opacity-80 p-2 rounded">
                    üéØ Jewelry Visible
                  </div>
                )}
              </div>

              <div className="flex gap-3">
                <button
                  onClick={stopCamera}
                  className="flex-1 border border-gray-300 hover:border-gray-400 py-3 rounded-lg font-medium transition flex items-center justify-center gap-2"
                >
                  <RefreshCw size={20} />
                  Stop
                </button>

                <button
                  onClick={capturePhoto}
                  disabled={!videoReady || !renderingActive}
                  className="flex-1 bg-indigo-600 hover:bg-indigo-700 text-white py-3 rounded-lg font-medium transition flex items-center justify-center gap-2 disabled:opacity-50"
                >
                  <Download size={20} />
                  Save Photo
                </button>
              </div>
            </div>
          )}
        </div>
      </div>
    </div>
  );
}
